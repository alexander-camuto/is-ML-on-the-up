# Is deep learning getting better ? 

Compilation of papers demonstrating that sometimes older is better.
Most of these papers show, usually empirically, that old neural network architectures can match and sometimes outperform current state of the art architectures if their hyper-parameters are tuned properly.

## Papers

- [Revisiting ResNets: Improved Training and Scaling Strategies](https://arxiv.org/abs/2103.07579), Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph (2021)
- [Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images](https://arxiv.org/abs/2011.10650), Rewon Child (2021)
- [An Evaluation of Change Point Detection Algorithms](https://arxiv.org/abs/2003.06222), Gerrit J.J. van den Burg, Christopher K.I. Williams (2020)
- [A Metric Learning Reality Check](https://arxiv.org/abs/2003.08505v1), Kevin Musgrave, Serge Belongie, Ser-Nam Lim (2020)
- [Unbiased Evaluation of Deep Metric Learning Algorithms](https://arxiv.org/abs/1911.12528), Istvan Fehervari, Avinash Ravichandran, Srikar Appalaraju (2019)
- [nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation](https://arxiv.org/abs/1809.10486), Fabian Isensee, Jens Petersen, Andre Klein, David Zimmerer, Paul F. Jaeger, Simon Kohl, Jakob Wasserthal, Gregor Koehler, Tobias Norajitra, Sebastian Wirkert, Klaus H. Maier-Hein (2018)
