# Is deep learning getting better ? 

Compilation of papers demonstrating that sometimes older is better.
Most of these papers demonstrate, usually empirically, that old neural network architectures can match and sometimes outperform current state of the art architectures if their hyper-parameters are tuned properly.
Some argue more broadly that how we evaluate ML models is borked -- real-world usage is so far from the academic setting that it is hard to glean much from published papers.

## Papers

- [Revisiting ResNets: Improved Training and Scaling Strategies](https://arxiv.org/abs/2103.07579), Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph (2021)
- [An Evaluation of Change Point Detection Algorithms](https://arxiv.org/abs/2003.06222), Gerrit J.J. van den Burg, Christopher K.I. Williams (2020)
- [A Metric Learning Reality Check](https://arxiv.org/abs/2003.08505v1), Kevin Musgrave, Serge Belongie, Ser-Nam Lim (2020)
- [Unbiased Evaluation of Deep Metric Learning Algorithms](https://arxiv.org/abs/1911.12528), Istvan Fehervari, Avinash Ravichandran, Srikar Appalaraju (2019)
- [Realistic Evaluation of Deep Semi-Supervised Learning Algorithms](https://arxiv.org/abs/1804.09170), Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk & Ian J. Goodfellow (2019)
